{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Load libraries and Defining fuctions for each stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import html\n",
    "\n",
    "# Import and initialize tqdm for Pandas\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import LdaModel, Nmf, CoherenceModel\n",
    "import pyLDAvis\n",
    "\n",
    "# Depress DeprecationWarnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import os.path\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from dvclive import Live\n",
    "import dvc.api\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 `preprocess(input_path)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_path, pct=1, RANDOM_SEED=42):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    if input_path.suffix == \".csv\":\n",
    "        df = pd.read_csv(input_path)\n",
    "    elif input_path.suffix == '.jsonl':\n",
    "        df = pd.read_json(input_path, lines=True)\n",
    "    elif input_path.suffix == '.parquet':\n",
    "        df = pd.read_parquet(input_path)\n",
    "    elif input_path.suffix == '.feather':\n",
    "        df = pd.read_feather(input_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown file type: {input_path.suffix}\")\n",
    "    \n",
    "    # Extract a subset of the data\n",
    "    if pct < 1:\n",
    "        df = df.sample(frac=pct, random_state=RANDOM_SEED)\n",
    "    \n",
    "    # Decode HTML entities back to original characters and remove whitespaces\n",
    "    df['review'] = df['review'].apply(html.unescape).str.replace(r'[\\r\\n\\t]', '', regex=True).str.strip()\n",
    "\n",
    "    # Remove wrong condition values and keep the rows\n",
    "    df.loc[df.condition.notna() & df.condition.str.contains('users found this comment helpful'), 'condition'] = None\n",
    "    \n",
    "    # Remove rows with empty reviews\n",
    "    df = df[df['review'].notna()]\n",
    "    df = df[df['review'] != '\"-\"']\n",
    "    df = df[df['review'] != '']\n",
    "\n",
    "    # Generate lemmas for each token, remove stopwords and punctuations\n",
    "    #df['procd_review'] = df['review'].progress_apply(\n",
    "    #    #lambda x: ' '.join([token.lemma_ for token in nlp(x) if not token.is_stop and not token.is_punct])\n",
    "    #    lambda x: [token.lemma_ for token in nlp(x) if not token.is_stop and not token.is_punct]\n",
    "    #)\n",
    "    def lemma(row):\n",
    "        # Skip if review is empty\n",
    "        if pd.isnull(row['review']): return row\n",
    "\n",
    "        # lemma_w_stpwrd: with stop words, for word2vec and bert embeddings\n",
    "        row['lemma_w_stpwrd'] = [token.lemma_ for token in nlp(row['review']) if not (token.is_punct or token.is_space or token.lemma_.strip() == '')]\n",
    "        # lemma_wo_stpwrd: lower without stop words, for BoW and TF-IDF embeddings\n",
    "        row['lemma_wo_stpwrd'] = [token.lemma_.lower() for token in nlp(row['review']) if not (token.is_stop or token.is_punct or token.is_space or token.lemma_.strip() == '')]\n",
    "        \n",
    "        # For reviews with only stop words, use lemma_w_stpwrd\n",
    "        if len(row['lemma_wo_stpwrd']) == 0:\n",
    "            row['lemma_wo_stpwrd'] = row['lemma_w_stpwrd']\n",
    "        return row\n",
    "    \n",
    "    procd_df = df.progress_apply(lemma, axis=1)\n",
    "\n",
    "    return procd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 `feature_engineering(procd_df, procd_text, ngram, bert, bert_pretrained_model, RANDOM_SEED)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(procd_data, ngram, bert, bert_pretrained_model=None, RANDOM_SEED=42):\n",
    "    # BERT Embeddings\n",
    "    if bert:\n",
    "        tokenizer = BertTokenizer.from_pretrained(bert_pretrained_model)\n",
    "        bert_model = BertModel.from_pretrained(bert_pretrained_model)\n",
    "        # Move model to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "            torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "        #elif torch.backends.mps.is_available():\n",
    "        #    device = torch.device('mps')\n",
    "        #    torch.mps.manual_seed(RANDOM_SEED)\n",
    "        #    torch.backends.mps.deterministic = True\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "            torch.manual_seed(RANDOM_SEED)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        bert_model = bert_model.to(device)\n",
    "        print(f\"Using device: {device}\")\n",
    "    \n",
    "        def get_bert_embeddings(text):\n",
    "            inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "            # Move inputs to GPU if available\n",
    "            if device.type != 'cpu':\n",
    "                inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "            outputs = bert_model(**inputs)\n",
    "            return outputs.last_hidden_state.mean(dim=1)\n",
    "        \n",
    "        bert_embeddings = [\n",
    "            get_bert_embeddings(doc).cpu().detach().numpy() for doc in tqdm(procd_data, desc='Generating BERT Embeddings')\n",
    "        ]\n",
    "    \n",
    "    # Extract BOW and TF-IDF features\n",
    "    # Add bigrams\n",
    "    if ngram == 'bigram':\n",
    "        phrase_model = Phrases(procd_data, min_count=1, threshold=1, connector_words=ENGLISH_CONNECTOR_WORDS)\n",
    "        procd_data_bigram = procd_data.progress_apply(lambda x: phrase_model[x])\n",
    "\n",
    "        dictionary = Dictionary(procd_data_bigram)\n",
    "    elif ngram == 'unigram':\n",
    "        dictionary = Dictionary(procd_data)\n",
    "\n",
    "    # BoW\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in tqdm(procd_data, desc='Generating BoW')]\n",
    "\n",
    "    # TF-IDF\n",
    "    tfidf_model = TfidfModel(bow_corpus)\n",
    "    tfidf_corpus = [tfidf_model[doc] for doc in tqdm(bow_corpus, desc='Generating TF-IDF')]\n",
    "    \n",
    "    return len(procd_data), bert_embeddings if bert else None, procd_data_bigram if ngram == 'bigram' else None, dictionary, bow_corpus, tfidf_corpus\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 `clustering(bert_embeddings, algorithm, num_clusters, RANDOM_SEED)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(bert_embeddings, algorithm, num_clusters, RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Cluster the input data using the specified algorithm and number of clusters.\n",
    "    \"\"\"\n",
    "    # Prepare the input data for clustering\n",
    "    bert_embedding_avg = [np.mean(embedding, axis=0) for embedding in bert_embeddings]\n",
    "    input_data = np.vstack(bert_embedding_avg)\n",
    "\n",
    "    # Scale the input data\n",
    "    scaler = StandardScaler()\n",
    "    input_data = scaler.fit_transform(input_data)\n",
    "\n",
    "    # Initialize clustering algorithm\n",
    "    if algorithm == 'kmeans':\n",
    "        clustering_model = KMeans(\n",
    "            n_clusters=num_clusters,\n",
    "            n_init='auto',\n",
    "            random_state=RANDOM_SEED,\n",
    "        )\n",
    "    elif algorithm == 'hierarchical':\n",
    "        clustering_model = AgglomerativeClustering(\n",
    "            n_clusters=num_clusters,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f'Unknown clustering algorithm: {algorithm}')\n",
    "\n",
    "    # Fit the clustering algorithm to the data and get the labels\n",
    "    clustering_model.fit(input_data)\n",
    "    labels = clustering_model.labels_\n",
    "\n",
    "    # Calculate the metrics if possible\n",
    "    if len(set(labels)) > 1:\n",
    "        silhouette = silhouette_score(input_data, labels)\n",
    "        davies_bouldin = davies_bouldin_score(input_data, labels)\n",
    "        calinski_harabasz = calinski_harabasz_score(input_data, labels)\n",
    "    else:\n",
    "        silhouette_avg = davies_bouldin = calinski_harabasz = np.nan\n",
    "\n",
    "    return clustering_model, silhouette, davies_bouldin, calinski_harabasz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 `topic_modeling(procd_data, corpus, dictionary, algorithm, num_topics, RANDOM_SEED)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modeling(procd_data, corpus, dictionary, algorithm, num_topics, RANDOM_SEED):\n",
    "    # Set up topic model\n",
    "    # LDA Model\n",
    "    if algorithm == 'lda':\n",
    "        topic_model = LdaModel(\n",
    "            corpus,\n",
    "            num_topics=num_topics,\n",
    "            id2word=dictionary,\n",
    "            random_state=RANDOM_SEED,\n",
    "        )\n",
    "        perplexity = topic_model.log_perplexity(corpus)\n",
    "    \n",
    "    # NMF Model\n",
    "    elif algorithm == 'nmf':\n",
    "        topic_model = Nmf(\n",
    "            corpus,\n",
    "            num_topics=num_topics,\n",
    "            id2word=dictionary,\n",
    "            random_state=RANDOM_SEED,\n",
    "        )\n",
    "        perplexity = None\n",
    "\n",
    "    # Calculate Coherence score\n",
    "    coherence_model = CoherenceModel(\n",
    "        model=topic_model,\n",
    "        texts=procd_data.tolist(),\n",
    "        #corpus=corpus,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v',\n",
    "        #coherence='u_mass',\n",
    "        processes=-1\n",
    "    )\n",
    "    coherence = coherence_model.get_coherence()\n",
    "\n",
    "    return topic_model, perplexity, coherence\n",
    "\n",
    "\n",
    "def prepare_topic_model_viz(topic_model, dictionary, corpus):\n",
    "    # Extract the topic-term matrix\n",
    "    topic_term_matrix = topic_model.get_topics()\n",
    "\n",
    "    # Extract the document-topic matrix\n",
    "    num_topics = topic_model.num_topics\n",
    "    doc_topic_matrix = []\n",
    "\n",
    "    for doc in tqdm(topic_model[corpus]):\n",
    "        doc_topics = dict(doc)\n",
    "        doc_topic_vec = [doc_topics.get(i, 0.0) for i in range(num_topics)]\n",
    "        doc_topic_matrix.append(doc_topic_vec)\n",
    "\n",
    "    # Normalize topic_term_matrix and doc_topic_matrix\n",
    "    topic_term_matrix = topic_term_matrix / np.sum(topic_term_matrix, axis=1, keepdims=True)\n",
    "    doc_topic_matrix = doc_topic_matrix / np.sum(doc_topic_matrix, axis=1, keepdims=True)\n",
    "\n",
    "    doc_topic_matrix = np.array(doc_topic_matrix)\n",
    "    \n",
    "    # Vocabulary and term frequencies\n",
    "    vocab = [dictionary[i] for i in range(len(dictionary))]\n",
    "\n",
    "    term_freq = np.zeros(len(vocab))\n",
    "    for doc in corpus:\n",
    "        for idx, freq in doc:\n",
    "            term_freq[idx] += freq\n",
    "    \n",
    "    # Prepare the data in pyLDAvis format\n",
    "    vis_data = pyLDAvis.prepare(\n",
    "        doc_lengths=np.array([sum(dict(doc).values()) for doc in corpus]),\n",
    "        vocab=vocab,\n",
    "        term_frequency=term_freq,\n",
    "        topic_term_dists=topic_term_matrix,\n",
    "        doc_topic_dists=doc_topic_matrix\n",
    "    )\n",
    "\n",
    "    # Return the visualization data\n",
    "    return vis_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 `pipeline()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(procd_df, procd_text, ngram, bert_pretrained_model, clustering, clustering_algorithms=None, num_clusters=None, feature='lda', topic_modeling_algorithm='lda', num_topics=8, RANDOM_SEED=42):\n",
    "    # Load preprocessed text data\n",
    "    procd_data = procd_df[procd_text].progress_apply(lambda x: eval(x))\n",
    "    procd_data = procd_data[procd_data.progress_apply(lambda row: len(row) > 0)]\n",
    "    \n",
    "    # Feature engineering\n",
    "    bert = clustering\n",
    "    num_docs, bert_embeddings, procd_data_bigram, dictionary, bow_corpus, tfidf_corpus = feature_engineering(\n",
    "        procd_data,\n",
    "        ngram,\n",
    "        bert,\n",
    "        bert_pretrained_model,\n",
    "        RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    # Clustering\n",
    "    if clustering:\n",
    "        clustering_model, silhouette, davies_bouldin, calinski_harabasz = clustering(\n",
    "            bert_embeddings,\n",
    "            clustering_algorithms,\n",
    "            num_clusters,\n",
    "            RANDOM_SEED\n",
    "        )\n",
    "    \n",
    "    # Topic Modeling\n",
    "    if feature == 'bow': corpus = bow_corpus\n",
    "    elif feature == 'tfidf': corpus = tfidf_corpus\n",
    "    else: raise ValueError(f'Unknown feature: {feature}')\n",
    "\n",
    "    if clustering:\n",
    "        # Add cluster labels to corpus\n",
    "        print(f\"Topic modeling with clustering via Bert {clustering_algorithms}...\")\n",
    "        # Get the all the cluster labels\n",
    "        labels = clustering_model.labels_\n",
    "\n",
    "        # Add cluster labels to the preprocessed text data\n",
    "        grouped_procd_data = pd.DataFrame({'cluster_label': labels, 'procd_text': procd_data_bigram}).groupby('cluster_label')\n",
    "\n",
    "        # Apply LDA to each clustered corpus\n",
    "        topic_models = {}\n",
    "        coherence_scores = {}\n",
    "        perplexity_scores = {}\n",
    "\n",
    "        for label, group in tqdm(grouped_procd_data, desc='Training topic models for each cluster'):\n",
    "            # Extract the clustered corpus and texts\n",
    "            clustered_corpus = [corpus[i] for i in group.index]\n",
    "            clustered_texts = group['procd_text']\n",
    "            \n",
    "            # Train the topic model for this cluster\n",
    "            topic_model, perplexity, coherence = topic_modeling(clustered_texts, clustered_corpus, dictionary, topic_modeling_algorithm, RANDOM_SEED)\n",
    "            \n",
    "            # Save the topic model\n",
    "            topic_models[label] = topic_model\n",
    "            \n",
    "            # Save the scores\n",
    "            coherence_scores[label] = coherence\n",
    "            if perplexity is not None: perplexity_scores[label] = perplexity\n",
    "\n",
    "        coherence = np.mean(list(coherence_scores.values()))\n",
    "        if len(perplexity_scores) == 0: perplexity = None\n",
    "        else: perplexity = np.mean(list(perplexity_scores.values()))\n",
    "    \n",
    "    else:\n",
    "        print('Topic modeling without clustering...')\n",
    "        topic_model, perplexity, coherence = topic_modeling(procd_data_bigram, corpus, dictionary, topic_modeling_algorithm, num_topics, RANDOM_SEED)\n",
    "    \n",
    "    return num_docs, clustering_model if clustering else None, silhouette if clustering else None, calinski_harabasz if clustering else None, davies_bouldin if clustering else None, topic_models if clustering else topic_model, coherence, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Set up path and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "train_data_path = '../data/raw/lewtun-drug-reviews/train.jsonl'\n",
    "\n",
    "# Feature Engineering\n",
    "procd_texts = ['lemma_wo_stpwrd', 'lemma_w_stpwrd']\n",
    "ngram = 'bigram'\n",
    "feature = 'tfidf'\n",
    "#bert_pretrained_model = 'bert-base-uncased'\n",
    "\n",
    "# Clustering\n",
    "clustering = False\n",
    "#clustering_algorithms = ['kmeans', 'hierarchical']\n",
    "#num_clusters = 2\n",
    "\n",
    "# Topic Modeling\n",
    "topic_modeling_algorithm = 'lda'\n",
    "nums_topics = [6, 8, 10, 12, 14, 16, 18, 20]\n",
    "\n",
    "# Model Settings\n",
    "models = [\n",
    "    {'procd_text': 'lemma_w_stpwrd', 'num_topics': 18},\n",
    "    {'procd_text': 'lemma_w_stpwrd', 'num_topics': 16},\n",
    "    {'procd_text': 'lemma_w_stpwrd', 'num_topics': 14},\n",
    "    {'procd_text': 'lemma_w_stpwrd', 'num_topics': 10},\n",
    "    {'procd_text': 'lemma_w_stpwrd', 'num_topics': 6},\n",
    "    {'procd_text': 'lemma_wo_stpwrd', 'num_topics': 16}\n",
    "    #{'procd_text': 'lemma_w_stpwrd', 'clustering': True, 'clustering_algorithms': 'kmeans', 'num_clusters': 2, 'topic_modeling_algorithm': 'lda', 'num_topics': 8},\n",
    "    #{'procd_text': 'lemma_wo_stpwrd', 'clustering': True, 'clustering_algorithms': 'hierarchical', 'num_clusters': 3, 'topic_modeling_algorithm': 'lda', 'num_topics': 12}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Preprocess train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "procd_df_path = '../cross_validation/preprocessed/procd_train.csv'\n",
    "\n",
    "if os.path.isfile(procd_df_path): procd_df = pd.read_csv(procd_df_path)\n",
    "else:\n",
    "    procd_df = preprocess(Path(train_data_path), pct=0.01, RANDOM_SEED=RANDOM_SEED)\n",
    "    procd_df.to_csv(procd_df_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:dvclive:Error in save_dvc_exp:  output '..\\data\\preprocessed\\procd_sample_train.csv' is already tracked by SCM (e.g. Git).\n",
      "    You can remove it from Git, then add to DVC.\n",
      "        To stop tracking from Git:\n",
      "            git rm -r --cached '..\\data\\preprocessed\\procd_sample_train.csv'\n",
      "            git commit -m \"stop tracking ..\\data\\preprocessed\\procd_sample_train.csv\" \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'procd_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sean7x\\medicinal-narratives\\notebooks\\cross_validation.ipynb Cell 21\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sean7x/medicinal-narratives/notebooks/cross_validation.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_procd_df \u001b[39m=\u001b[39m procd_df\u001b[39m.\u001b[39miloc[train]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sean7x/medicinal-narratives/notebooks/cross_validation.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m test_procd_df \u001b[39m=\u001b[39m procd_df\u001b[39m.\u001b[39miloc[test]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sean7x/medicinal-narratives/notebooks/cross_validation.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m procd_text \u001b[39min\u001b[39;00m tqdm(procd_texts, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPreprocessed Text: \u001b[39m\u001b[39m{\u001b[39;00mprocd_text\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sean7x/medicinal-narratives/notebooks/cross_validation.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# Load preprocessed text data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sean7x/medicinal-narratives/notebooks/cross_validation.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     train_procd_data \u001b[39m=\u001b[39m train_procd_df[procd_text]\u001b[39m.\u001b[39mprogress_apply(\u001b[39mlambda\u001b[39;00m x: \u001b[39meval\u001b[39m(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sean7x/medicinal-narratives/notebooks/cross_validation.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     train_procd_data \u001b[39m=\u001b[39m train_procd_data[train_procd_data\u001b[39m.\u001b[39mprogress_apply(\u001b[39mlambda\u001b[39;00m row: \u001b[39mlen\u001b[39m(row) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'procd_text' is not defined"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "with Live(dir='../cross_validation') as live:\n",
    "    for fold, (train, test) in enumerate(kf.split(procd_df)):\n",
    "        train_procd_df = procd_df.iloc[train]\n",
    "        test_procd_df = procd_df.iloc[test]\n",
    "\n",
    "        for procd_text in tqdm(procd_texts, desc=f'Preprocessed Text:'):\n",
    "            print(procd_text)\n",
    "            # Load preprocessed text data\n",
    "            train_procd_data = train_procd_df[procd_text].progress_apply(lambda x: eval(x))\n",
    "            train_procd_data = train_procd_data[train_procd_data.progress_apply(lambda row: len(row) > 0)]\n",
    "\n",
    "            test_procd_data = test_procd_df[procd_text].progress_apply(lambda x: eval(x))\n",
    "            test_procd_data = test_procd_data[test_procd_data.progress_apply(lambda row: len(row) > 0)]\n",
    "            \n",
    "            # Extract features from the training data\n",
    "            train_num_docs, _, procd_data_bigram, dictionary, train_bow_corpus, train_tfidf_corpus = feature_engineering(\n",
    "                procd_data = train_procd_data,\n",
    "                ngram = ngram,\n",
    "                bert = False,\n",
    "                bert_pretrained_model = None,\n",
    "                RANDOM_SEED = RANDOM_SEED\n",
    "            )\n",
    "\n",
    "            # Extract features from the test data\n",
    "            #_, _, _, _, test_bow_corpus, test_tfidf_corpus = feature_engineering(\n",
    "            #    procd_data = test_procd_data,\n",
    "            #    ngram = ngram,\n",
    "            #    bert = False,\n",
    "            #    bert_pretrained_model = None,\n",
    "            #    RANDOM_SEED = RANDOM_SEED\n",
    "            #)\n",
    "            \n",
    "            for num_topics in tqdm(nums_topics, desc=f'Training topic models'):\n",
    "                # Train the topic model\n",
    "                if feature == 'bow': train_corpus = train_bow_corpus\n",
    "                elif feature == 'tfidf': train_corpus = train_tfidf_corpus\n",
    "                else: raise ValueError(f'Unknown feature: {feature}')\n",
    "\n",
    "                topic_model, perplexity, coherence = topic_modeling(\n",
    "                    procd_data_bigram, train_corpus, dictionary, topic_modeling_algorithm, num_topics, RANDOM_SEED\n",
    "                )\n",
    "\n",
    "                live.log_param('Preprocessed Text', procd_text)\n",
    "                live.log_param('ngram', ngram)\n",
    "                live.log_param('Feature', feature)\n",
    "                live.log_param('Topic Modeling Algorithm', topic_modeling_algorithm)\n",
    "                live.log_param('Num of Topics', num_topics)\n",
    "\n",
    "                live.log_metric('Fold', fold)\n",
    "                live.log_metric('Num of Docs (Train)', train_num_docs)\n",
    "                live.log_metric('Coherence (Train)', coherence)\n",
    "                if perplexity is not None: live.log_metric('Perplexity (Train)', perplexity)\n",
    "\n",
    "                live.log_artifact('Topic Model', topic_model, f'topic_model_{fold}_{procd_text}_{num_topics}.pkl')\n",
    "\n",
    "                # Evaluate the model\n",
    "                #if feature == 'bow': test_corpus = test_bow_corpus\n",
    "                #elif feature == 'tfidf': test_corpus = test_tfidf_corpus\n",
    "                #else: raise ValueError(f'Unknown feature: {feature}')\n",
    "\n",
    "                # Calculate Coherence score\n",
    "                coherence_model = CoherenceModel(\n",
    "                    model=topic_model,\n",
    "                    texts=test_procd_data.tolist(),\n",
    "                    #corpus=test_corpus,\n",
    "                    dictionary=dictionary,\n",
    "                    coherence='c_v',\n",
    "                    #coherence='u_mass',\n",
    "                    processes=-1\n",
    "                )\n",
    "                coherence = coherence_model.get_coherence()\n",
    "                live.log_metric('Coherence (Test)', coherence)\n",
    "\n",
    "                live.next_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
