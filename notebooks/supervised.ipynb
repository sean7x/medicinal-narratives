{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import dvc.api\n",
    "\n",
    "from dvclive import Live\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(procd_df, tfidf_corpus, topics_dist, bert_embeddings=None):\n",
    "    from textblob import TextBlob\n",
    "\n",
    "     # Add review length\n",
    "    procd_df['review_length'] = procd_df['review'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    # Add sentiment score\n",
    "    procd_df['sentiment_score'] = procd_df['review'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "    # Assert that the length of the features are equal to the length of the data\n",
    "    assert (len(procd_df) == len(tfidf_corpus) == len(topics_dist)), \\\n",
    "        print(f\"\"\"procd_df: {len(procd_df)} \\n\n",
    "              tfidf_corpus: {len(tfidf_corpus)} \\n  \n",
    "              topics_dist: {len(topics_dist)}\"\"\")\n",
    "    \n",
    "    # Add tfidf of reviews\n",
    "    procd_df['tfidf'] = tfidf_corpus\n",
    "\n",
    "    # Add topic distribution of revies\n",
    "    procd_df = pd.concat([procd_df, topics_dist], axis=1)\n",
    "\n",
    "    # Add bert for reviews\n",
    "    #if bert_embeddings:\n",
    "    #    procd_df['bert'] = bert_embeddings\n",
    "    \n",
    "    return procd_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(X_train, X_test, y_train, y_test, features, model, params):\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "    from scipy.sparse import csr_matrix, vstack, hstack\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Scale the features\n",
    "    final_features_csr_train = []\n",
    "    final_features_csr_test = []\n",
    "    for feature in features:\n",
    "        scaler = StandardScaler(with_mean=False)\n",
    "        if feature == 'tfidf':\n",
    "            #scaler = MinMaxScaler()\n",
    "            # Determine the maximum index across both training and test sets\n",
    "            max_index = max(\n",
    "                max(idx for vec in X_train[feature] for idx, _ in vec),\n",
    "                max(idx for vec in X_test[feature] for idx, _ in vec)\n",
    "            )\n",
    "\n",
    "            batch_size = 1000\n",
    "\n",
    "            # Function to convert a batch of Gensim-style TF-IDF vectors to a CSR matrix\n",
    "            def gensim_to_csr(batch, num_terms):\n",
    "                data, rows, cols = [], [], []\n",
    "                for i, doc in enumerate(batch):\n",
    "                    for term_id, value in doc:\n",
    "                        rows.append(i)\n",
    "                        cols.append(term_id)\n",
    "                        data.append(value)\n",
    "                return csr_matrix((data, (rows, cols)), shape=(len(batch), num_terms))\n",
    "            \n",
    "\n",
    "            # Scale and convert the TF-IDF vectors for train and test datasets\n",
    "            for data in [X_train, X_test]:\n",
    "                n_batches = int(np.ceil(len(data) / batch_size))\n",
    "                scaled_csr_matrices = []\n",
    "\n",
    "                for i in tqdm(range(n_batches), total=n_batches, desc='Scaling tfidf'):\n",
    "                    start_idx = i * batch_size\n",
    "                    end_idx = (i + 1) * batch_size\n",
    "                    batch = data.iloc[start_idx:end_idx]\n",
    "                    \n",
    "                    # Convert the batch to a CSR matrix\n",
    "                    batch_csr_matrix = gensim_to_csr(batch[feature], max_index + 1)\n",
    "\n",
    "                    if data is X_train:\n",
    "                        # Use toarray() as MinMaxScaler's partial_fit doesn't support sparse matrix\n",
    "                        scaler.partial_fit(batch_csr_matrix.toarray())\n",
    "                        scaled_batch = scaler.transform(batch_csr_matrix)\n",
    "                    else:\n",
    "                        scaled_batch = scaler.transform(batch_csr_matrix)\n",
    "\n",
    "                    scaled_csr_matrices.append(scaled_batch)\n",
    "\n",
    "                # Vertically stack the CSR matrices to get the final scaled matrix\n",
    "                final_csr_matrix = vstack(scaled_csr_matrices)\n",
    "                if data is X_train:\n",
    "                    final_features_csr_train.append(final_csr_matrix)\n",
    "                else:\n",
    "                    final_features_csr_test.append(final_csr_matrix)\n",
    "        else:\n",
    "            final_features_csr_train.append(csr_matrix(scaler.fit_transform(X_train[feature].values.reshape(-1, 1))))\n",
    "            final_features_csr_test.append(csr_matrix(scaler.transform(X_test[feature].values.reshape(-1, 1))))\n",
    "            #X_test.loc[:, feature] = scaler.transform(X_test[feature].values.reshape(-1, 1))\n",
    "\n",
    "    X_train = hstack(final_features_csr_train)\n",
    "    X_test = hstack(final_features_csr_test)\n",
    "    # Initialize the model\n",
    "    if model == 'linear':\n",
    "        reg = LinearRegression()\n",
    "    elif model == 'randomforest':\n",
    "        reg = RandomForestRegressor()\n",
    "    elif model == 'svm':\n",
    "        reg = SVR()\n",
    "    \n",
    "    # Train the model\n",
    "    reg.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the ratings\n",
    "    y_pred = reg.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Log the metrics\n",
    "    #print(f\"\"\"MSE: {mse} \\n\n",
    "    #        MAE: {mae} \\n\n",
    "    #        R2: {r2}\"\"\")\n",
    "\n",
    "    return mse, mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(procd_df, params):\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import KFold\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Set features\n",
    "    topic_columns = [column for column in procd_df.columns if 'Topic' in column]\n",
    "    features = ['review_length', 'sentiment_score', 'tfidf'] + topic_columns\n",
    "    #features = ['tfidf']\n",
    "    #features = topic_columns\n",
    "\n",
    "    # Set target\n",
    "    target = 'rating'\n",
    "\n",
    "    # Cross validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=params['RANDOM_SEED'])\n",
    "\n",
    "    # Initialize metrics\n",
    "    mses = []\n",
    "    maes = []\n",
    "    r2s = []\n",
    "\n",
    "    for train_index, test_index in tqdm(kf.split(procd_df), total=kf.get_n_splits(), desc='Cross validation'):\n",
    "        # Split the data\n",
    "        train, test = procd_df.iloc[train_index], procd_df.iloc[test_index]\n",
    "        X_train, X_test = train[features], test[features]\n",
    "        y_train, y_test = train[target], test[target]\n",
    "\n",
    "        # Predict the ratings\n",
    "        fold_mses = []\n",
    "        fold_maes = []\n",
    "        fold_r2s = []\n",
    "        for model in tqdm(['linear', 'randomforest', 'svm']):\n",
    "            mse, mae, r2 = predict_rating(X_train, X_test, y_train, y_test, features, model, params)\n",
    "\n",
    "            fold_mses.append({model: mse})\n",
    "            fold_maes.append({model: mae})\n",
    "            fold_r2s.append({model: r2})\n",
    "\n",
    "        # Append the metrics\n",
    "        mses.append(fold_mses)\n",
    "        maes.append(fold_maes)\n",
    "        r2s.append(fold_r2s)\n",
    "    \n",
    "    return mses, maes, r2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters from DVC\n",
    "params = dvc.api.params_show()\n",
    "\n",
    "# Set paths\n",
    "procd_data_path = '../data/preprocessed/procd_train.csv'\n",
    "tfidf_corpus_path = f\"../data/features/{params['procd_text']}/{params['topic_modeling']['feature']}_{params['feature_engineering']['ngram']}_corpus.pkl\"\n",
    "topics_dist_path = f\"../data/evaluate/topics_dist_train.csv\"\n",
    "\n",
    "bert_embeddings_path = f\"../data/features/bert_embeddings.pkl\"\n",
    "\n",
    "# Load preprocessed data\n",
    "procd_df = pd.read_csv(procd_data_path)\n",
    "procd_df[params['procd_text']] = procd_df[params['procd_text']].apply(lambda x: eval(x))\n",
    "procd_df = procd_df[procd_df[params['procd_text']].apply(lambda row: len(row) > 0)]\n",
    "\n",
    "# Load tfidf corpus\n",
    "tfidf_corpus = pickle.load(open(tfidf_corpus_path, 'rb'))\n",
    "\n",
    "# Load Bert embeddings\n",
    "#bert_embeddings = pickle.load(open(bert_embeddings_path, 'rb'))\n",
    "\n",
    "# Load topic distributions\n",
    "topics_dist = pd.read_csv(topics_dist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "procd_df_with_features = add_features(procd_df, tfidf_corpus, topics_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross validation:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "\n",
      "Scaling tfidf:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Scaling tfidf:  25%|██▌       | 1/4 [00:02<00:08,  2.85s/it]\n",
      "\n",
      "Scaling tfidf:  50%|█████     | 2/4 [00:05<00:05,  2.79s/it]\n",
      "\n",
      "Scaling tfidf:  75%|███████▌  | 3/4 [00:08<00:02,  2.72s/it]\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 4/4 [00:10<00:00,  2.64s/it]\n",
      "\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 1/1 [00:00<00:00, 123.38it/s]\n",
      "\n",
      " 33%|███▎      | 1/3 [00:19<00:38, 19.31s/it]\n",
      "\n",
      "Scaling tfidf:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\n",
      "Scaling tfidf:  25%|██▌       | 1/4 [00:02<00:07,  2.51s/it]\n",
      "\n",
      "Scaling tfidf:  50%|█████     | 2/4 [00:05<00:05,  2.52s/it]\n",
      "\n",
      "Scaling tfidf:  75%|███████▌  | 3/4 [00:07<00:02,  2.54s/it]\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 4/4 [00:09<00:00,  2.47s/it]\n",
      "\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 1/1 [00:00<00:00, 124.95it/s]\n",
      "\n",
      " 67%|██████▋   | 2/3 [08:53<05:10, 310.51s/it]\n",
      "\n",
      "Scaling tfidf:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\n",
      "Scaling tfidf:  25%|██▌       | 1/4 [00:02<00:07,  2.51s/it]\n",
      "\n",
      "Scaling tfidf:  50%|█████     | 2/4 [00:05<00:05,  2.55s/it]\n",
      "\n",
      "Scaling tfidf:  75%|███████▌  | 3/4 [00:07<00:02,  2.55s/it]\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 4/4 [00:09<00:00,  2.48s/it]\n",
      "\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 1/1 [00:00<00:00, 125.01it/s]\n",
      "\n",
      "100%|██████████| 3/3 [09:06<00:00, 182.32s/it]\n",
      "Cross validation:  20%|██        | 1/5 [09:06<36:27, 546.96s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "\n",
      "Scaling tfidf:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\n",
      "Scaling tfidf:  25%|██▌       | 1/4 [00:02<00:07,  2.55s/it]\n",
      "\n",
      "Scaling tfidf:  50%|█████     | 2/4 [00:05<00:05,  2.54s/it]\n",
      "\n",
      "Scaling tfidf:  75%|███████▌  | 3/4 [00:07<00:02,  2.55s/it]\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 4/4 [00:09<00:00,  2.47s/it]\n",
      "\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 1/1 [00:00<00:00, 116.98it/s]\n",
      "\n",
      " 33%|███▎      | 1/3 [00:18<00:37, 18.93s/it]\n",
      "\n",
      "Scaling tfidf:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\n",
      "Scaling tfidf:  25%|██▌       | 1/4 [00:02<00:07,  2.52s/it]\n",
      "\n",
      "Scaling tfidf:  50%|█████     | 2/4 [00:05<00:05,  2.56s/it]\n",
      "\n",
      "Scaling tfidf:  75%|███████▌  | 3/4 [00:07<00:02,  2.56s/it]\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 4/4 [00:09<00:00,  2.48s/it]\n",
      "\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 1/1 [00:00<00:00, 111.12it/s]\n",
      "\n",
      " 67%|██████▋   | 2/3 [08:47<05:07, 307.23s/it]\n",
      "\n",
      "Scaling tfidf:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\n",
      "Scaling tfidf:  25%|██▌       | 1/4 [00:02<00:08,  2.90s/it]\n",
      "\n",
      "Scaling tfidf:  50%|█████     | 2/4 [00:05<00:05,  2.97s/it]\n",
      "\n",
      "Scaling tfidf:  75%|███████▌  | 3/4 [00:08<00:02,  2.94s/it]\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 4/4 [00:11<00:00,  2.81s/it]\n",
      "\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 1/1 [00:00<00:00, 111.13it/s]\n",
      "\n",
      "100%|██████████| 3/3 [09:02<00:00, 180.91s/it]\n",
      "Cross validation:  40%|████      | 2/5 [18:09<27:13, 544.48s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "\n",
      "Scaling tfidf:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\n",
      "Scaling tfidf:  25%|██▌       | 1/4 [00:02<00:08,  2.81s/it]\n",
      "\n",
      "Scaling tfidf:  50%|█████     | 2/4 [00:05<00:05,  2.77s/it]\n",
      "\n",
      "Scaling tfidf:  75%|███████▌  | 3/4 [00:08<00:02,  2.69s/it]\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 4/4 [00:10<00:00,  2.64s/it]\n",
      "\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 1/1 [00:00<00:00, 110.82it/s]\n",
      "\n",
      " 33%|███▎      | 1/3 [00:19<00:39, 19.79s/it]\n",
      "\n",
      "Scaling tfidf:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\n",
      "Scaling tfidf:  25%|██▌       | 1/4 [00:02<00:08,  2.82s/it]\n",
      "\n",
      "Scaling tfidf:  50%|█████     | 2/4 [00:05<00:05,  2.77s/it]\n",
      "\n",
      "Scaling tfidf:  75%|███████▌  | 3/4 [00:08<00:02,  2.71s/it]\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 4/4 [00:10<00:00,  2.67s/it]\n",
      "\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 1/1 [00:00<00:00, 99.76it/s]\n",
      "\n",
      " 67%|██████▋   | 2/3 [08:55<05:11, 311.65s/it]\n",
      "\n",
      "Scaling tfidf:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\n",
      "Scaling tfidf:  25%|██▌       | 1/4 [00:02<00:07,  2.64s/it]\n",
      "\n",
      "Scaling tfidf:  50%|█████     | 2/4 [00:05<00:05,  2.60s/it]\n",
      "\n",
      "Scaling tfidf:  75%|███████▌  | 3/4 [00:07<00:02,  2.58s/it]\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 4/4 [00:10<00:00,  2.52s/it]\n",
      "\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 1/1 [00:00<00:00, 110.81it/s]\n",
      "\n",
      "100%|██████████| 3/3 [09:09<00:00, 183.04s/it]\n",
      "Cross validation:  60%|██████    | 3/5 [27:18<18:13, 546.60s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "\n",
      "Scaling tfidf:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\n",
      "Scaling tfidf:  25%|██▌       | 1/4 [00:02<00:07,  2.53s/it]\n",
      "\n",
      "Scaling tfidf:  50%|█████     | 2/4 [00:05<00:05,  2.58s/it]\n",
      "\n",
      "Scaling tfidf:  75%|███████▌  | 3/4 [00:07<00:02,  2.58s/it]\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 4/4 [00:09<00:00,  2.49s/it]\n",
      "\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 1/1 [00:00<00:00, 90.92it/s]\n",
      "\n",
      " 33%|███▎      | 1/3 [00:21<00:42, 21.20s/it]\n",
      "\n",
      "Scaling tfidf:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\n",
      "Scaling tfidf:  25%|██▌       | 1/4 [00:02<00:07,  2.62s/it]\n",
      "\n",
      "Scaling tfidf:  50%|█████     | 2/4 [00:05<00:05,  2.61s/it]\n",
      "\n",
      "Scaling tfidf:  75%|███████▌  | 3/4 [00:07<00:02,  2.63s/it]\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]\n",
      "\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 1/1 [00:00<00:00, 124.70it/s]\n",
      "\n",
      " 67%|██████▋   | 2/3 [09:09<05:19, 319.47s/it]\n",
      "\n",
      "Scaling tfidf:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\n",
      "Scaling tfidf:  25%|██▌       | 1/4 [00:02<00:07,  2.62s/it]\n",
      "\n",
      "Scaling tfidf:  50%|█████     | 2/4 [00:05<00:05,  2.62s/it]\n",
      "\n",
      "Scaling tfidf:  75%|███████▌  | 3/4 [00:07<00:02,  2.62s/it]\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 4/4 [00:10<00:00,  2.53s/it]\n",
      "\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 1/1 [00:00<00:00, 110.81it/s]\n",
      "\n",
      "100%|██████████| 3/3 [09:23<00:00, 187.67s/it]\n",
      "Cross validation:  80%|████████  | 4/5 [36:41<09:13, 553.09s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "\n",
      "Scaling tfidf:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\n",
      "Scaling tfidf:  25%|██▌       | 1/4 [00:02<00:07,  2.60s/it]\n",
      "\n",
      "Scaling tfidf:  50%|█████     | 2/4 [00:05<00:05,  2.62s/it]\n",
      "\n",
      "Scaling tfidf:  75%|███████▌  | 3/4 [00:07<00:02,  2.61s/it]\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 4/4 [00:10<00:00,  2.52s/it]\n",
      "\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 1/1 [00:00<00:00, 124.99it/s]\n",
      "\n",
      " 33%|███▎      | 1/3 [00:19<00:38, 19.17s/it]\n",
      "\n",
      "Scaling tfidf:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\n",
      "Scaling tfidf:  25%|██▌       | 1/4 [00:02<00:07,  2.56s/it]\n",
      "\n",
      "Scaling tfidf:  50%|█████     | 2/4 [00:05<00:05,  2.61s/it]\n",
      "\n",
      "Scaling tfidf:  75%|███████▌  | 3/4 [00:07<00:02,  2.59s/it]\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 4/4 [00:10<00:00,  2.52s/it]\n",
      "\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 1/1 [00:00<00:00, 109.82it/s]\n",
      "\n",
      " 67%|██████▋   | 2/3 [08:58<05:13, 313.32s/it]\n",
      "\n",
      "Scaling tfidf:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\n",
      "Scaling tfidf:  25%|██▌       | 1/4 [00:02<00:07,  2.60s/it]\n",
      "\n",
      "Scaling tfidf:  50%|█████     | 2/4 [00:05<00:05,  2.62s/it]\n",
      "\n",
      "Scaling tfidf:  75%|███████▌  | 3/4 [00:07<00:02,  2.62s/it]\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 4/4 [00:10<00:00,  2.53s/it]\n",
      "\n",
      "\n",
      "Scaling tfidf: 100%|██████████| 1/1 [00:00<00:00, 111.11it/s]\n",
      "\n",
      "100%|██████████| 3/3 [09:12<00:00, 184.02s/it]\n",
      "Cross validation: 100%|██████████| 5/5 [45:53<00:00, 550.78s/it]\n"
     ]
    }
   ],
   "source": [
    "sample_df = procd_df_with_features.sample(frac=0.03, random_state=params['RANDOM_SEED'])\n",
    "\n",
    "mses, maes, r2s = cross_validate(sample_df, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for name,metric in zip(['mse', 'mae', 'r'], [mses, maes, r2s]):\n",
    "    df = pd.DataFrame(metric)\n",
    "    df.columns = ['linear', 'randomforest', 'svm']\n",
    "\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: x.get(col, None))\n",
    "    df = pd.melt(df, var_name='model', value_name=name)\n",
    "    \n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_metric_df = dfs[0].copy()\n",
    "lr_metric_df['mae'] = dfs[1]['mae']\n",
    "lr_metric_df['r2'] = dfs[2]['r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear</td>\n",
       "      <td>2593.356946</td>\n",
       "      <td>40.726120</td>\n",
       "      <td>-240.989061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>linear</td>\n",
       "      <td>4386.943000</td>\n",
       "      <td>48.639193</td>\n",
       "      <td>-396.784429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linear</td>\n",
       "      <td>2671.050219</td>\n",
       "      <td>40.229343</td>\n",
       "      <td>-252.678234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>linear</td>\n",
       "      <td>4926.532751</td>\n",
       "      <td>52.324096</td>\n",
       "      <td>-479.534075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>linear</td>\n",
       "      <td>2854.608776</td>\n",
       "      <td>42.090570</td>\n",
       "      <td>-274.781428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>randomforest</td>\n",
       "      <td>8.710707</td>\n",
       "      <td>2.287634</td>\n",
       "      <td>0.187194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>randomforest</td>\n",
       "      <td>10.071522</td>\n",
       "      <td>2.454514</td>\n",
       "      <td>0.086768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>randomforest</td>\n",
       "      <td>9.707454</td>\n",
       "      <td>2.402975</td>\n",
       "      <td>0.078052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>randomforest</td>\n",
       "      <td>9.013452</td>\n",
       "      <td>2.351364</td>\n",
       "      <td>0.120828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>randomforest</td>\n",
       "      <td>9.174936</td>\n",
       "      <td>2.342420</td>\n",
       "      <td>0.113617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>svm</td>\n",
       "      <td>11.875432</td>\n",
       "      <td>2.536907</td>\n",
       "      <td>-0.108110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svm</td>\n",
       "      <td>12.670331</td>\n",
       "      <td>2.630109</td>\n",
       "      <td>-0.148878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>svm</td>\n",
       "      <td>11.453562</td>\n",
       "      <td>2.495498</td>\n",
       "      <td>-0.087782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>svm</td>\n",
       "      <td>10.918630</td>\n",
       "      <td>2.458775</td>\n",
       "      <td>-0.065003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>svm</td>\n",
       "      <td>11.781794</td>\n",
       "      <td>2.544102</td>\n",
       "      <td>-0.138230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model          mse        mae          r2\n",
       "0         linear  2593.356946  40.726120 -240.989061\n",
       "1         linear  4386.943000  48.639193 -396.784429\n",
       "2         linear  2671.050219  40.229343 -252.678234\n",
       "3         linear  4926.532751  52.324096 -479.534075\n",
       "4         linear  2854.608776  42.090570 -274.781428\n",
       "5   randomforest     8.710707   2.287634    0.187194\n",
       "6   randomforest    10.071522   2.454514    0.086768\n",
       "7   randomforest     9.707454   2.402975    0.078052\n",
       "8   randomforest     9.013452   2.351364    0.120828\n",
       "9   randomforest     9.174936   2.342420    0.113617\n",
       "10           svm    11.875432   2.536907   -0.108110\n",
       "11           svm    12.670331   2.630109   -0.148878\n",
       "12           svm    11.453562   2.495498   -0.087782\n",
       "13           svm    10.918630   2.458775   -0.065003\n",
       "14           svm    11.781794   2.544102   -0.138230"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr_metric_df.to_csv('../data/evaluate/lr_metrics.csv', index=False)\n",
    "lr_metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
